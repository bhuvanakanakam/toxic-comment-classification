{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_5mgFjKUrl",
        "outputId": "4ad97bde-9a32-49c5-ab79-a82743a0f195"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zARLmopSG9pg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "try:\n",
        "  from indicnlp.tokenize import indic_tokenize\n",
        "except ImportError:\n",
        "  print(\"indic-nlp library not found. Skipping tokenization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "x_jdG4JeHDrA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re"
      ],
      "metadata": {
        "id": "GRSYUimGMvxb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_8-aKa1Kw4x",
        "outputId": "4217a329-14ad-4260-9683-ba72e4303e6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_train.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_val.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_test.csv\")"
      ],
      "metadata": {
        "id": "R5kvYOx4HG-p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nValidation Dataset Sample:\")\n",
        "print(val_df.head())\n",
        "\n",
        "print(\"\\nTest Dataset Sample:\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j01YaozPK0iL",
        "outputId": "b98c876a-67fc-4379-f5aa-8953c4f81aef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Sample:\n",
            "   label                                       text\n",
            "0      0              ‡§≠‡•Ä‡§°‡§º ‡§Æ‡•á‡§Ç  ‡§¨‡§π‡•Å‡§§  ‡§π‡§¨‡•ç‡§∏‡•Ä ‡§Æ‡§ø‡§≤‡•á‡§Ç‡§ó‡•á\n",
            "1      0  ‡§∏‡§æ‡§≤‡•á ‡§¨‡•á‡§µ‡§ï‡•Ç‡§´ ‡§Ö‡§™‡§®‡•Ä ‡§Æ‡§æ‡§Ç ‡§Æ‡§ï‡•ç‡§ñ‡§ø‡§Ø‡§æ‡§Ç  ‡§§‡•ã ‡§π‡§ü‡§æ ‡§¶‡•á‡§Ç\n",
            "2      0           ‡§¨‡•Å‡§∞ ‡§¶‡•á‡§¶‡•ã ‡§§‡•ã ‡§Æ‡•Å‡§π ‡§Æ‡•á‡§Ç ‡§≤‡§Ç‡§° ‡§≤‡•á ‡§≤‡•ã ‡§§‡•ã\n",
            "3      0       ‡§ï‡•Å‡§§‡•ç‡§§‡§æ ‡§µ‡§π‡§æ ‡§π‡•à ‡§ö‡§ø‡§≤‡•ç‡§≤‡§æ ‡§§‡•Ç ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§∞‡§π‡§æ ‡§π‡•à\n",
            "4      1  ‡§ö‡§æ‡§Ø ‡§®‡§π‡•Ä‡§Ç ‡§™‡•Ä‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§Æ‡•à‡§Ç ‡§á‡§∏‡•Ä ‡§ï‡•ã ‡§õ‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ok\n",
            "\n",
            "Validation Dataset Sample:\n",
            "   label                                               text\n",
            "0      0                          Comment box ‡§ö‡§æ‡§≤‡•Ç ‡§ï‡§∞ ‡§ù‡§µ‡§æ‡§°‡•á\n",
            "1      0  ‡§Ü‡§™‡§ï‡•á ‡§™‡§æ‡§∏ ‡§™‡•Å‡§ñ‡•ç‡§§‡§æ ‡§∏‡§¨‡•Ç‡§§ ‡§π‡•à, ‡§ï‡•ç‡§Ø‡§æ ‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§ï‡•Å‡§õ ‡§≠‡•Ä‡•§ ‡§â...\n",
            "2      1  üë¨‡§¶‡•ã‡§∏‡•ç‡§§‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‚Äì One Time üôÉ ‡§π‡§Æ ‡§®‡§ø‡§≠‡§æ‡§§‡•á ‡§π‡•à ‚Äì So...\n",
            "3      1  ‡§ö‡§æ‡§Ø ‡§≤‡§µ‡§∞ ‡§°‡§æ‡•Ö. ‡§ï‡•ã ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§ó‡•á or mo. ‡§ï‡•ã‡§à ‡§õ‡•ã‡§°‡§º ‡§®‡§π‡•Ä‡§Ç...\n",
            "4      1  ‡§ú‡•ã ‡§Ö‡§®‡§™‡§¢‡§º ‡§≤‡•ã‡§ó ‡§¶‡§ø‡§®-‡§∞‡§æ‡§§ ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§ï‡§∞‡§§‡•á ‡§∞‡§π‡§§‡•á ‡§π...\n",
            "\n",
            "Test Dataset Sample:\n",
            "   label                                               text\n",
            "0      0  ‡§Æ‡•à‡§Ç ‡§Ø‡•á ‡§®‡§π‡•Ä ‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§ï‡•Ä ‡§á‡§∏‡•á ‡§®‡§ø‡§ï‡§≤‡•á ‡§ï‡•à‡§∏‡•á ‡§Æ‡•à‡§Ç ‡§Ø‡•á ‡§∏‡•ã...\n",
            "1      1        ‡§î‡§∞ ‡§¶‡§ø‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§™‡•Ç‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§™‡§°‡§º‡§æ‡§ï‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§´‡•ã‡§°‡§æ‡§§\n",
            "2      1      ‡§ï‡•Å‡§§‡•ç‡§§‡§æ ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§™‡§æ‡§≤ ‡§≤‡•á‡§®‡§æ ‡§Æ‡§ó‡§∞ ‡§ó‡§≤‡§§ ‡§´‡§π‡§Æ‡•Ä ‡§ï‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç‡•§\n",
            "3      0      ‡§§‡•á‡§∞‡•Ä ‡§ó‡§æ‡§Ç‡§° ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§Ø‡§æ‡§ú ‡§ï‡§æ‡§ü ‡§¶‡•á‡§ó‡§æ ‡§ó‡•Å‡§ú‡•ç‡§ú‡§∞ ‡§≠‡•ã‡§∏‡§°‡§º‡•Ä ‡§ï‡•á\n",
            "4      1            ‡§¨‡§Ç‡§ó‡§æ‡§≤‡•Ä ‡§∏‡§æ‡§°‡§º‡•Ä ‡§ê‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§π‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§¶‡•Ä‡§¶‡•Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Columns:\")\n",
        "print(train_df.columns)\n",
        "\n",
        "print(\"\\nValidation Dataset Columns:\")\n",
        "print(val_df.columns)\n",
        "\n",
        "print(\"\\nTest Dataset Columns:\")\n",
        "print(test_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bABjLIwYK2x-",
        "outputId": "5752191d-b13e-449e-dc4a-4383e99b10e8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n",
            "\n",
            "Validation Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n",
            "\n",
            "Test Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_file = '/content/drive/My Drive/Data/final_stopwords.txt'  # Replace with the path to your stopwords file\n",
        "with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
        "    stop_words_list = [line.strip() for line in file]\n",
        "stop_words_hindi = set(stop_words_list)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words_hindi])\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
        "train_df['tokens'] = train_df['text'].apply(indic_tokenize.trivial_tokenize)"
      ],
      "metadata": {
        "id": "6fTA_0SQK5cR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()  # Lowercase\n",
        "  text = re.sub(r'[^\\w\\s\\u0900-\\u097F]', ' ', text)  # Keeps Hindi characters, whitespace, and alphanumeric\n",
        "  text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "  # Apply stopword removal if stopwords_file is defined\n",
        "  if stopwords_file:\n",
        "      text = remove_stopwords(text)\n",
        "  # Tokenization (using indic_nlp if available)\n",
        "  try:\n",
        "      text = indic_tokenize.trivial_tokenize(text)\n",
        "      text = ' '.join(text)  # Join tokens back into string\n",
        "  except ImportError:\n",
        "      print(\"indic_nlp library not found. Skipping tokenization.\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "oGOsCRRGK6EO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Training Data:\")\n",
        "train_df[\"text_preprocessed\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Training Text:\")\n",
        "print(train_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeRDEoq9K72m",
        "outputId": "e48e6c55-a0ff-4704-d360-d4d8d097f345"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Training Data:\n",
            "Sample Preprocessed Training Text:\n",
            "0                  ‡§≠‡•Ä‡§°‡§º ‡§π‡§¨‡•ç‡§∏‡•Ä ‡§Æ‡§ø‡§≤‡•á‡§Ç‡§ó‡•á\n",
            "1    ‡§∏‡§æ‡§≤‡•á ‡§¨‡•á‡§µ‡§ï‡•Ç‡§´ ‡§Æ‡§æ‡§Ç ‡§Æ‡§ï‡•ç‡§ñ‡§ø‡§Ø‡§æ‡§Ç ‡§π‡§ü‡§æ ‡§¶‡•á‡§Ç\n",
            "2              ‡§¨‡•Å‡§∞ ‡§¶‡•á‡§¶‡•ã ‡§Æ‡•Å‡§π ‡§≤‡§Ç‡§° ‡§≤‡•á ‡§≤‡•ã\n",
            "3                ‡§ï‡•Å‡§§‡•ç‡§§‡§æ ‡§µ‡§π‡§æ ‡§ö‡§ø‡§≤‡•ç‡§≤‡§æ ‡§§‡•Ç\n",
            "4           ‡§ö‡§æ‡§Ø ‡§®‡§π‡•Ä‡§Ç ‡§™‡•Ä‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§õ‡•ã‡§°‡§º ok\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Validation Data:\")\n",
        "val_df[\"text_preprocessed\"] = val_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Validation Text:\")\n",
        "print(val_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrV7aab_K-J8",
        "outputId": "476fbc2e-1ad4-40a6-da6e-6eba230d7b88"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Validation Data:\n",
            "Sample Preprocessed Validation Text:\n",
            "0                               comment box ‡§ö‡§æ‡§≤‡•Ç ‡§ù‡§µ‡§æ‡§°‡•á\n",
            "1    ‡§Ü‡§™‡§ï‡•á ‡§™‡§æ‡§∏ ‡§™‡•Å‡§ñ‡•ç‡§§‡§æ ‡§∏‡§¨‡•Ç‡§§ ‡§≠‡•Ä ‡•§ ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§° ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§µ...\n",
            "2    ‡§¶‡•ã‡§∏‡•ç‡§§‡•Ä one time ‡§®‡§ø‡§≠‡§æ‡§§‡•á some time ‡§Ø‡§æ‡§¶ ‡§ï‡§∞‡•ã any t...\n",
            "3    ‡§ö‡§æ‡§Ø ‡§≤‡§µ‡§∞ ‡§°‡§æ‡•Ö ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§ó‡•á or mo ‡§õ‡•ã‡§°‡§º ‡§®‡§π‡•Ä‡§Ç ‡§ì‡§∞ m bhi...\n",
            "4    ‡§Ö‡§®‡§™‡§¢‡§º ‡§≤‡•ã‡§ó ‡§¶‡§ø‡§® ‡§∞‡§æ‡§§ ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§∞‡§π‡§§‡•á ‡§∏‡•ã‡§ö‡§®‡§æ ‡§¶‡•á‡§ñ...\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Test Data:\")\n",
        "test_df[\"text_preprocessed\"] = test_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Test Text:\")\n",
        "print(test_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQv9KApoLAbV",
        "outputId": "4af07c1b-0b14-4bca-c3e6-b84fac3b473e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Test Data:\n",
            "Sample Preprocessed Test Text:\n",
            "0                       ‡§®‡§π‡•Ä ‡§∏‡•ã‡§ö ‡§®‡§ø‡§ï‡§≤‡•á ‡§∏‡•ã‡§ö ‡§´‡§Ç‡§∏‡§æ\n",
            "1                 ‡§¶‡§ø‡§µ‡§æ‡§≤‡•Ä ‡§¶‡•á‡§∂ ‡§™‡§°‡§º‡§æ‡§ï‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§´‡•ã‡§°‡§æ‡§§\n",
            "2       ‡§ï‡•Å‡§§‡•ç‡§§‡§æ ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§™‡§æ‡§≤ ‡§≤‡•á‡§®‡§æ ‡§ó‡§≤‡§§ ‡§´‡§π‡§Æ‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡•§\n",
            "3    ‡§§‡•á‡§∞‡•Ä ‡§ó‡§æ‡§Ç‡§° ‡§™‡•ç‡§Ø‡§æ‡§ú ‡§ï‡§æ‡§ü ‡§¶‡•á‡§ó‡§æ ‡§ó‡•Å‡§ú‡•ç‡§ú‡§∞ ‡§≠‡•ã‡§∏‡§°‡§º‡•Ä ‡§ï‡•á\n",
            "4                  ‡§¨‡§Ç‡§ó‡§æ‡§≤‡•Ä ‡§∏‡§æ‡§°‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§™‡§π‡§®‡§æ ‡§¶‡•Ä‡§¶‡•Ä\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei0TCOShLEba",
        "outputId": "b97e829e-d085-4836-e971-a3e8947c7afa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class ToxicCommentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "at4j8MCALS-q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Datasets and Dataloaders\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = ToxicCommentDataset(\n",
        "        texts=df[\"text_preprocessed\"].tolist(),\n",
        "        labels=df[\"label\"].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "rH9OWccdOzZe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Functions\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels.extend(batch['labels'].numpy())\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "    return accuracy_score(labels, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0jDBHs2O2H6",
        "outputId": "68501b8c-a23f-49e8-b61f-2eb7d94ce0e1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_epoch(model, train_data_loader, optimizer, device)\n",
        "    val_accuracy = evaluate(model, val_data_loader, device)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print(f'Train loss: {train_loss}')\n",
        "    print(f'Validation accuracy: {val_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYDA9xPhO5_2",
        "outputId": "456a5f2b-d762-4031-fe52-3f5ad5daafb2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Train loss: 0.4767965018194281\n",
            "Validation accuracy: 0.8317479191438764\n",
            "Epoch 2/5\n",
            "Train loss: 0.35736179521835737\n",
            "Validation accuracy: 0.8315992865636147\n",
            "Epoch 3/5\n",
            "Train loss: 0.28990627370520883\n",
            "Validation accuracy: 0.8434898929845422\n",
            "Epoch 4/5\n",
            "Train loss: 0.230724264415964\n",
            "Validation accuracy: 0.8347205707491082\n",
            "Epoch 5/5\n",
            "Train loss: 0.17441970582242625\n",
            "Validation accuracy: 0.832936979785969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Test Set\n",
        "test_accuracy = evaluate(model, test_data_loader, device)\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZcr5irHVsb6",
        "outputId": "4c8cd031-3f27-4d84-bb0d-0576aa21614e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8376932223543401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get predictions for a sample of data\n",
        "def get_predictions(model, data_loader, device):\n",
        "    model.eval()\n",
        "    texts = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels.extend(batch['labels'].cpu().numpy())\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "            # Decode input_ids to get the original text\n",
        "            texts.extend([tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids])\n",
        "    return texts, preds, labels\n",
        "\n",
        "# Get predictions for the test set\n",
        "test_texts, test_preds, test_labels = get_predictions(model, test_data_loader, device)\n",
        "\n",
        "# Display a sample of the predictions\n",
        "sample_size = 10  # Number of samples to display\n",
        "print(f'Displaying {sample_size} samples from the test set predictions:\\n')\n",
        "\n",
        "for i in range(sample_size):\n",
        "    print(f'Text: {test_texts[i]}')\n",
        "    print(f'Actual Label: {test_labels[i]}')\n",
        "    print(f'Predicted Label: {test_preds[i]}')\n",
        "    print('---')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZfgKsmLWBGs",
        "outputId": "4ce97f02-18d2-4ea6-f76d-5a97406a85a4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 10 samples from the test set predictions:\n",
            "\n",
            "Text: ‡§™‡•ç‡§Ø‡§æ‡§∞‡•Ä ‡§≤‡§ó‡§∞‡§π‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§∏‡§®‡§Æ ‡§Ü‡§Ø ‡§≤‡§µ‡•ç‡§π ‡§Ø‡•Å ‡§∏‡§Ç‡§ö‡§ø‡§§‡§æ\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: ‡§Ö‡§∞‡•á ‡§™‡§æ‡§ó‡§≤ ‡§î‡§∞‡§§ ‡§π‡§∞ ‡§µ‡§ø‡§°‡§ø‡§Ø‡•ã ‡§è‡§ï‡§ü‡§ø‡§ó‡§Ç ‡§ï‡§∞‡•á‡§ó‡•Ä ‡§ï ‡§Ö‡§≤‡§ó ‡§≤‡•á\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç ‡§¶‡•Ä‡§™‡§æ‡§µ‡§≤‡•Ä ‡§π‡§æ‡§∞‡•ç‡§¶‡§ø‡§ï ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç ‡§à‡§∂‡•ç‡§µ‡§∞ ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ ‡§µ‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§π‡§Æ‡•á‡§∂‡§æ ‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§ä‡§∞‡•ç‡§ú‡§æ‡§µ‡§æ‡§®\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: ‡§≤‡§Ç‡§ó‡§∞ mae ‡§¨‡•ã‡§ü‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§§‡•Ä chutiye\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: ‡§≠‡§æ‡§à ‡§≤‡•ã‡§ó ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§ñ‡•ã 2 ‡§ò‡•ã‡§°‡§º‡•á ‡§ó‡§ß‡§æ\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: ‡§á‡§® ‡•§ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§µ‡§æ‡§≤‡•ã ‡§ï‡•Å‡§§‡•ç‡§§‡•ã‡§Ç ‡§á‡§§‡§®‡•Ä ‡§π‡§ø‡§Æ‡•ç‡§Æ‡§§ ‡§∏‡§Æ‡§ù ‡§≤‡•ã ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§®‡•å‡§ï‡§∞‡•Ä ‡§π‡§æ‡§• ‡§ß‡•ã ‡§¨‡•à‡§†‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ú‡§µ‡§æ‡§® ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§Æ‡§æ‡§∏‡•ç‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§≤‡§ó‡§æ‡§®‡•á ‡§á‡§§‡§®‡•Ä ‡§¨‡•Å‡§∞‡•Ä ‡§™‡§ø‡§ü‡§æ‡§à\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: ‡§≤‡•ã‡§ó ‡§¶‡§¨‡§Ç‡§ó ‡§ú‡•Ä ‡§§‡•Å‡§ù‡•á ‡§≤‡§ó‡§§‡§æ ‡§õ‡•ã‡§ü‡•Ä ‡§õ‡•ã‡§ü‡•Ä ‡§ö‡•Ä‡§ú‡•ã‡§Ç ‡§¶‡§ø‡§ñ‡§æ ‡§≤‡•ã‡§ó ‡§™‡§æ‡§ó‡§≤ ‡§¶‡•á‡§ó‡•Ä ‡§á‡§∂ ‡§õ‡•ã‡§ü‡•Ä ‡§∏‡•Ä ‡§ö‡•Ä‡§ú 100 ‡§∞‡•Å‡§™‡§Ø‡§æ ‡§ñ‡•á‡§≤ ‡§Ü\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: ‡§ú‡§Ø ‡§∞‡§æ‡§Æ ‡§¶‡•á‡§µ ‡§¨‡§æ‡§¨‡§æ\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: ‡§®‡§æ ‡§∂‡§∞‡§Æ‡§æ‡§ì ‡§≤‡§æ‡§ú‡§µ‡§æ‡§¨ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§ï‡§∞‡§ï‡•á ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§¨‡§æ‡§ï‡•Ä 70 ‡§∏‡§æ‡§≤ ‡§ö‡•Å‡§ü‡§ø‡§Ø‡§æ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§¨‡§®‡§®‡•á ‡§¨‡•à‡§†‡•á\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}