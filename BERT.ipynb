{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_5mgFjKUrl",
        "outputId": "4ad97bde-9a32-49c5-ab79-a82743a0f195"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zARLmopSG9pg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "try:\n",
        "  from indicnlp.tokenize import indic_tokenize\n",
        "except ImportError:\n",
        "  print(\"indic-nlp library not found. Skipping tokenization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "x_jdG4JeHDrA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re"
      ],
      "metadata": {
        "id": "GRSYUimGMvxb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_8-aKa1Kw4x",
        "outputId": "4217a329-14ad-4260-9683-ba72e4303e6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_train.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_val.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/Data/hindi_test.csv\")"
      ],
      "metadata": {
        "id": "R5kvYOx4HG-p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nValidation Dataset Sample:\")\n",
        "print(val_df.head())\n",
        "\n",
        "print(\"\\nTest Dataset Sample:\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j01YaozPK0iL",
        "outputId": "b98c876a-67fc-4379-f5aa-8953c4f81aef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Sample:\n",
            "   label                                       text\n",
            "0      0              भीड़ में  बहुत  हब्सी मिलेंगे\n",
            "1      0  साले बेवकूफ अपनी मां मक्खियां  तो हटा दें\n",
            "2      0           बुर देदो तो मुह में लंड ले लो तो\n",
            "3      0       कुत्ता वहा है चिल्ला तू क्यों रहा है\n",
            "4      1  चाय नहीं पीता हूं मैं इसी को छोड़ दिया ok\n",
            "\n",
            "Validation Dataset Sample:\n",
            "   label                                               text\n",
            "0      0                          Comment box चालू कर झवाडे\n",
            "1      0  आपके पास पुख्ता सबूत है, क्या या फिर कुछ भी। उ...\n",
            "2      1  👬दोस्ती होती है – One Time 🙃 हम निभाते है – So...\n",
            "3      1  चाय लवर डाॅ. को छोड़ देगे or mo. कोई छोड़ नहीं...\n",
            "4      1  जो अनपढ़ लोग दिन-रात हिंदू मुस्लिम करते रहते ह...\n",
            "\n",
            "Test Dataset Sample:\n",
            "   label                                               text\n",
            "0      0  मैं ये नही सोच रहा की इसे निकले कैसे मैं ये सो...\n",
            "1      1        और दिवाली में भी पूरा देश पड़ाका नहीं फोडात\n",
            "2      1      कुत्ता बिल्ली पाल लेना मगर गलत फहमी कभी नहीं।\n",
            "3      0      तेरी गांड में प्याज काट देगा गुज्जर भोसड़ी के\n",
            "4      1            बंगाली साड़ी ऐसे नहीं पहना जाता है दीदी\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Columns:\")\n",
        "print(train_df.columns)\n",
        "\n",
        "print(\"\\nValidation Dataset Columns:\")\n",
        "print(val_df.columns)\n",
        "\n",
        "print(\"\\nTest Dataset Columns:\")\n",
        "print(test_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bABjLIwYK2x-",
        "outputId": "5752191d-b13e-449e-dc4a-4383e99b10e8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n",
            "\n",
            "Validation Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n",
            "\n",
            "Test Dataset Columns:\n",
            "Index(['label', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_file = '/content/drive/My Drive/Data/final_stopwords.txt'  # Replace with the path to your stopwords file\n",
        "with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
        "    stop_words_list = [line.strip() for line in file]\n",
        "stop_words_hindi = set(stop_words_list)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words_hindi])\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
        "train_df['tokens'] = train_df['text'].apply(indic_tokenize.trivial_tokenize)"
      ],
      "metadata": {
        "id": "6fTA_0SQK5cR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()  # Lowercase\n",
        "  text = re.sub(r'[^\\w\\s\\u0900-\\u097F]', ' ', text)  # Keeps Hindi characters, whitespace, and alphanumeric\n",
        "  text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "  # Apply stopword removal if stopwords_file is defined\n",
        "  if stopwords_file:\n",
        "      text = remove_stopwords(text)\n",
        "  # Tokenization (using indic_nlp if available)\n",
        "  try:\n",
        "      text = indic_tokenize.trivial_tokenize(text)\n",
        "      text = ' '.join(text)  # Join tokens back into string\n",
        "  except ImportError:\n",
        "      print(\"indic_nlp library not found. Skipping tokenization.\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "oGOsCRRGK6EO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Training Data:\")\n",
        "train_df[\"text_preprocessed\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Training Text:\")\n",
        "print(train_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeRDEoq9K72m",
        "outputId": "e48e6c55-a0ff-4704-d360-d4d8d097f345"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Training Data:\n",
            "Sample Preprocessed Training Text:\n",
            "0                  भीड़ हब्सी मिलेंगे\n",
            "1    साले बेवकूफ मां मक्खियां हटा दें\n",
            "2              बुर देदो मुह लंड ले लो\n",
            "3                कुत्ता वहा चिल्ला तू\n",
            "4           चाय नहीं पीता हूं छोड़ ok\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Validation Data:\")\n",
        "val_df[\"text_preprocessed\"] = val_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Validation Text:\")\n",
        "print(val_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrV7aab_K-J8",
        "outputId": "476fbc2e-1ad4-40a6-da6e-6eba230d7b88"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Validation Data:\n",
            "Sample Preprocessed Validation Text:\n",
            "0                               comment box चालू झवाडे\n",
            "1    आपके पास पुख्ता सबूत भी । आधार कार्ड के कारण व...\n",
            "2    दोस्ती one time निभाते some time याद करो any t...\n",
            "3    चाय लवर डाॅ छोड़ देगे or mo छोड़ नहीं ओर m bhi...\n",
            "4    अनपढ़ लोग दिन रात हिंदू मुस्लिम रहते सोचना देख...\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Test Data:\")\n",
        "test_df[\"text_preprocessed\"] = test_df[\"text\"].apply(preprocess_text)\n",
        "print(\"Sample Preprocessed Test Text:\")\n",
        "print(test_df[\"text_preprocessed\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQv9KApoLAbV",
        "outputId": "4af07c1b-0b14-4bca-c3e6-b84fac3b473e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Test Data:\n",
            "Sample Preprocessed Test Text:\n",
            "0                       नही सोच निकले सोच फंसा\n",
            "1                 दिवाली देश पड़ाका नहीं फोडात\n",
            "2       कुत्ता बिल्ली पाल लेना गलत फहमी नहीं ।\n",
            "3    तेरी गांड प्याज काट देगा गुज्जर भोसड़ी के\n",
            "4                  बंगाली साड़ी नहीं पहना दीदी\n",
            "Name: text_preprocessed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei0TCOShLEba",
        "outputId": "b97e829e-d085-4836-e971-a3e8947c7afa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class ToxicCommentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "at4j8MCALS-q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Datasets and Dataloaders\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = ToxicCommentDataset(\n",
        "        texts=df[\"text_preprocessed\"].tolist(),\n",
        "        labels=df[\"label\"].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "rH9OWccdOzZe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Functions\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels.extend(batch['labels'].numpy())\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "    return accuracy_score(labels, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0jDBHs2O2H6",
        "outputId": "68501b8c-a23f-49e8-b61f-2eb7d94ce0e1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_epoch(model, train_data_loader, optimizer, device)\n",
        "    val_accuracy = evaluate(model, val_data_loader, device)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print(f'Train loss: {train_loss}')\n",
        "    print(f'Validation accuracy: {val_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYDA9xPhO5_2",
        "outputId": "456a5f2b-d762-4031-fe52-3f5ad5daafb2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Train loss: 0.4767965018194281\n",
            "Validation accuracy: 0.8317479191438764\n",
            "Epoch 2/5\n",
            "Train loss: 0.35736179521835737\n",
            "Validation accuracy: 0.8315992865636147\n",
            "Epoch 3/5\n",
            "Train loss: 0.28990627370520883\n",
            "Validation accuracy: 0.8434898929845422\n",
            "Epoch 4/5\n",
            "Train loss: 0.230724264415964\n",
            "Validation accuracy: 0.8347205707491082\n",
            "Epoch 5/5\n",
            "Train loss: 0.17441970582242625\n",
            "Validation accuracy: 0.832936979785969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Test Set\n",
        "test_accuracy = evaluate(model, test_data_loader, device)\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZcr5irHVsb6",
        "outputId": "4c8cd031-3f27-4d84-bb0d-0576aa21614e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8376932223543401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get predictions for a sample of data\n",
        "def get_predictions(model, data_loader, device):\n",
        "    model.eval()\n",
        "    texts = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels.extend(batch['labels'].cpu().numpy())\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "            # Decode input_ids to get the original text\n",
        "            texts.extend([tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids])\n",
        "    return texts, preds, labels\n",
        "\n",
        "# Get predictions for the test set\n",
        "test_texts, test_preds, test_labels = get_predictions(model, test_data_loader, device)\n",
        "\n",
        "# Display a sample of the predictions\n",
        "sample_size = 10  # Number of samples to display\n",
        "print(f'Displaying {sample_size} samples from the test set predictions:\\n')\n",
        "\n",
        "for i in range(sample_size):\n",
        "    print(f'Text: {test_texts[i]}')\n",
        "    print(f'Actual Label: {test_labels[i]}')\n",
        "    print(f'Predicted Label: {test_preds[i]}')\n",
        "    print('---')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZfgKsmLWBGs",
        "outputId": "4ce97f02-18d2-4ea6-f76d-5a97406a85a4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 10 samples from the test set predictions:\n",
            "\n",
            "Text: प्यारी लगरही तुम्हे प्यार सनम आय लव्ह यु संचिता\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: अरे पागल औरत हर विडियो एकटिगं करेगी क अलग ले\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: दोस्तों दीपावली हार्दिक शुभकामनाएं ईश्वर प्रार्थना वो आपको हमेशा स्वस्थ ऊर्जावान\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: लंगर mae बोटी नहीं मिलती chutiye\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: भाई लोग ध्यान देखो 2 घोड़े गधा\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: इन । पुलिस वालो कुत्तों इतनी हिम्मत समझ लो कुत्ते नौकरी हाथ धो बैठे कुत्ते जवान सिर्फ मास्क नहीं लगाने इतनी बुरी पिटाई\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: लोग दबंग जी तुझे लगता छोटी छोटी चीजों दिखा लोग पागल देगी इश छोटी सी चीज 100 रुपया खेल आ\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n",
            "Text: जय राम देव बाबा\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: ना शरमाओ लाजवाब सुन्दर\n",
            "Actual Label: 1\n",
            "Predicted Label: 1\n",
            "---\n",
            "Text: बीजेपी विश्वास करके दिखाया बाकी 70 साल चुटिया बनाया बनने बैठे\n",
            "Actual Label: 0\n",
            "Predicted Label: 0\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}